{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running a MLOps pipeline in MLServer\n",
    "\n",
    "This example walks you through how to create and serialise a [MLOps pipeline](https://github.com/SeldonIO/mlops), which can then be served through MLServer.\n",
    "This pipeline can contain custom Python arbitrary code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the pipeline\n",
    "\n",
    "The first step will be to create our MLOps pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from mlops.serve.metadata import (\n",
    "    ModelFramework,\n",
    "    MetadataTensorParameters,\n",
    "    MetadataTensor,\n",
    ")\n",
    "from mlops.serve.model import Model\n",
    "from mlops.serve.pipeline import Pipeline\n",
    "from mlops.serve.utils import pipeline\n",
    "from mlops.seldon.docker import SeldonDockerRuntime\n",
    "\n",
    "MODELS_PATH = os.path.join(os.getcwd(), 'models')\n",
    "\n",
    "docker_runtime = SeldonDockerRuntime()\n",
    "model_outputs = [\n",
    "    MetadataTensor(\n",
    "        name=\"output-0\",\n",
    "        datatype=\"FP32\",\n",
    "        shape=[2, 3],\n",
    "        parameters=MetadataTensorParameters(ext_datatype=np.ndarray),\n",
    "    )\n",
    "]\n",
    "\n",
    "sklearn_iris_path = os.path.join(MODELS_PATH, 'sklearn-iris')\n",
    "sklearn_model = Model(\n",
    "    name=\"test-iris-sklearn\",\n",
    "    runtime=docker_runtime,\n",
    "    framework=ModelFramework.SKLearn,\n",
    "    uri=\"gs://seldon-models/sklearn/iris\",\n",
    "    local_folder=sklearn_iris_path,\n",
    "    outputs=model_outputs\n",
    ")\n",
    "\n",
    "xgboost_iris_path = os.path.join(MODELS_PATH, 'xgboost-iris')\n",
    "xgboost_model = Model(\n",
    "    name=\"test-iris-xgboost\",\n",
    "    runtime=docker_runtime,\n",
    "    framework=ModelFramework.XGBoost,\n",
    "    uri=\"gs://seldon-models/xgboost/iris\",\n",
    "    local_folder=xgboost_iris_path,\n",
    "    outputs=model_outputs\n",
    ")\n",
    "\n",
    "@pipeline(name=\"inference-pipeline\", models=[sklearn_model, xgboost_model])\n",
    "def inference_pipeline(payload: np.ndarray) -> np.ndarray:\n",
    "    res1 = sklearn_model(payload)\n",
    "    if res1[0][0] > 0.7:\n",
    "        return res1\n",
    "    else:\n",
    "        return xgboost_model(payload)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This pipeline can then be serialised using `cloudpickle`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_pipeline.save(\"inference-pipeline.pickle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Serving the pipeline\n",
    "\n",
    "Once we have our pipeline created and serialised, we can then create a `model-settings.json` file.\n",
    "This configuration file will hold the configuration specific to our MLOps pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./model-settings.json\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./model-settings.json\n",
    "{\n",
    "    \"name\": \"mlops-pipeline\",\n",
    "    \"implementation\": \"mlserver_mlops.MLOpsModel\",\n",
    "    \"parameters\": {\n",
    "        \"uri\": \"./inference-pipeline.pickle\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start serving our model\n",
    "\n",
    "Now that we have our config in-place, we can start the server by running `mlserver start .`. This needs to either be ran from the same directory where our config files are or pointing to the folder where they are.\n",
    "\n",
    "```shell\n",
    "mlserver start .\n",
    "```\n",
    "\n",
    "Since this command will start the server and block the terminal, waiting for requests, this will need to be ran in the background on a separate terminal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy our pipeline components\n",
    "\n",
    "Additionally, we will also need to deploy our pipeline components.\n",
    "That is, the SKLearn and XGBoost models.\n",
    "We can do that as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_pipeline.deploy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Send test inference request\n",
    "\n",
    "We now have our model being served by `mlserver`.\n",
    "To make sure that everything is working as expected, let's send a request.\n",
    "\n",
    "For that, we can use the Python types that `mlserver` provides out of box, or we can build our request manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_name': 'mlops-pipeline',\n",
       " 'model_version': None,\n",
       " 'id': '91720186-2f8c-4ccd-8b5f-5ae460115efd',\n",
       " 'parameters': None,\n",
       " 'outputs': [{'name': 'predict',\n",
       "   'shape': [1, 3],\n",
       "   'datatype': 'FP32',\n",
       "   'parameters': None,\n",
       "   'data': [[0.8260199086532507, 0.038597383243756486, 0.1353827081029928]]}]}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "x_0 = np.array([[0.1, 3.1, 1.5, 0.2]])\n",
    "inference_request = {\n",
    "    \"inputs\": [\n",
    "        {\n",
    "          \"name\": \"predict\",\n",
    "          \"shape\": x_0.shape,\n",
    "          \"datatype\": \"FP32\",\n",
    "          \"data\": x_0.tolist()\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "endpoint = \"http://localhost:8080/v2/models/mlops-pipeline/infer\"\n",
    "response = requests.post(endpoint, json=inference_request)\n",
    "\n",
    "response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
